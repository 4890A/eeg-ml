{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kclk6hHetB03"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"#URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "OYkJo1n0tNul",
    "outputId": "051af054-31ad-4448-f310-0bc226fdc918"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>subject</th>\n",
       "      <th>FP1</th>\n",
       "      <th>FP2</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>AF1</th>\n",
       "      <th>AF2</th>\n",
       "      <th>FZ</th>\n",
       "      <th>F4</th>\n",
       "      <th>F3</th>\n",
       "      <th>FC6</th>\n",
       "      <th>FC5</th>\n",
       "      <th>FC2</th>\n",
       "      <th>FC1</th>\n",
       "      <th>T8</th>\n",
       "      <th>T7</th>\n",
       "      <th>CZ</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>CP5</th>\n",
       "      <th>CP6</th>\n",
       "      <th>CP1</th>\n",
       "      <th>CP2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "      <th>PZ</th>\n",
       "      <th>P8</th>\n",
       "      <th>P7</th>\n",
       "      <th>PO2</th>\n",
       "      <th>PO1</th>\n",
       "      <th>O2</th>\n",
       "      <th>O1</th>\n",
       "      <th>X</th>\n",
       "      <th>AF7</th>\n",
       "      <th>AF8</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>FT7</th>\n",
       "      <th>FT8</th>\n",
       "      <th>FPZ</th>\n",
       "      <th>FC4</th>\n",
       "      <th>FC3</th>\n",
       "      <th>C6</th>\n",
       "      <th>C5</th>\n",
       "      <th>F2</th>\n",
       "      <th>F1</th>\n",
       "      <th>TP8</th>\n",
       "      <th>TP7</th>\n",
       "      <th>AFZ</th>\n",
       "      <th>CP3</th>\n",
       "      <th>CP4</th>\n",
       "      <th>P5</th>\n",
       "      <th>P6</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>PO7</th>\n",
       "      <th>PO8</th>\n",
       "      <th>FCZ</th>\n",
       "      <th>POZ</th>\n",
       "      <th>OZ</th>\n",
       "      <th>P2</th>\n",
       "      <th>P1</th>\n",
       "      <th>CPZ</th>\n",
       "      <th>nd</th>\n",
       "      <th>Y</th>\n",
       "      <th>alcoholic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>co2a0000364</td>\n",
       "      <td>-8.921</td>\n",
       "      <td>0.834</td>\n",
       "      <td>-19.847</td>\n",
       "      <td>8.148</td>\n",
       "      <td>-2.146</td>\n",
       "      <td>1.129</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>3.408</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>4.832</td>\n",
       "      <td>-2.431</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.824</td>\n",
       "      <td>-3.886</td>\n",
       "      <td>-6.805</td>\n",
       "      <td>-2.716</td>\n",
       "      <td>-2.716</td>\n",
       "      <td>-0.926</td>\n",
       "      <td>10.396</td>\n",
       "      <td>5.096</td>\n",
       "      <td>-1.770</td>\n",
       "      <td>-0.834</td>\n",
       "      <td>-4.720</td>\n",
       "      <td>-2.340</td>\n",
       "      <td>-2.797</td>\n",
       "      <td>-3.815</td>\n",
       "      <td>-6.755</td>\n",
       "      <td>-6.266</td>\n",
       "      <td>-6.643</td>\n",
       "      <td>-7.477</td>\n",
       "      <td>-8.698</td>\n",
       "      <td>-5.269</td>\n",
       "      <td>-16.856</td>\n",
       "      <td>-10.020</td>\n",
       "      <td>-9.288</td>\n",
       "      <td>12.624</td>\n",
       "      <td>-10.162</td>\n",
       "      <td>-1.373</td>\n",
       "      <td>-1.231</td>\n",
       "      <td>-6.846</td>\n",
       "      <td>1.333</td>\n",
       "      <td>0.763</td>\n",
       "      <td>-8.586</td>\n",
       "      <td>0.529</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>-3.082</td>\n",
       "      <td>-9.338</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-1.851</td>\n",
       "      <td>-1.801</td>\n",
       "      <td>-5.412</td>\n",
       "      <td>-2.909</td>\n",
       "      <td>-1.129</td>\n",
       "      <td>2.747</td>\n",
       "      <td>3.906</td>\n",
       "      <td>5.157</td>\n",
       "      <td>1.048</td>\n",
       "      <td>-6.266</td>\n",
       "      <td>-9.033</td>\n",
       "      <td>-2.421</td>\n",
       "      <td>-4.313</td>\n",
       "      <td>-0.478</td>\n",
       "      <td>-8.901</td>\n",
       "      <td>-5.636</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>co2a0000364</td>\n",
       "      <td>-8.433</td>\n",
       "      <td>3.276</td>\n",
       "      <td>-12.522</td>\n",
       "      <td>1.801</td>\n",
       "      <td>-2.146</td>\n",
       "      <td>0.641</td>\n",
       "      <td>-0.559</td>\n",
       "      <td>1.455</td>\n",
       "      <td>0.397</td>\n",
       "      <td>6.297</td>\n",
       "      <td>-4.384</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>0.824</td>\n",
       "      <td>-5.839</td>\n",
       "      <td>-9.247</td>\n",
       "      <td>-2.716</td>\n",
       "      <td>-3.204</td>\n",
       "      <td>-2.879</td>\n",
       "      <td>20.162</td>\n",
       "      <td>5.585</td>\n",
       "      <td>-0.793</td>\n",
       "      <td>-1.322</td>\n",
       "      <td>-4.720</td>\n",
       "      <td>-3.805</td>\n",
       "      <td>-4.262</td>\n",
       "      <td>-4.791</td>\n",
       "      <td>-7.243</td>\n",
       "      <td>-7.731</td>\n",
       "      <td>-9.084</td>\n",
       "      <td>-11.383</td>\n",
       "      <td>-12.604</td>\n",
       "      <td>-5.758</td>\n",
       "      <td>-7.090</td>\n",
       "      <td>-7.090</td>\n",
       "      <td>-13.194</td>\n",
       "      <td>6.765</td>\n",
       "      <td>-10.162</td>\n",
       "      <td>-2.350</td>\n",
       "      <td>0.722</td>\n",
       "      <td>-5.870</td>\n",
       "      <td>0.356</td>\n",
       "      <td>-0.702</td>\n",
       "      <td>-11.515</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-4.059</td>\n",
       "      <td>-12.268</td>\n",
       "      <td>-1.475</td>\n",
       "      <td>0.590</td>\n",
       "      <td>-2.289</td>\n",
       "      <td>-5.412</td>\n",
       "      <td>-4.862</td>\n",
       "      <td>-1.617</td>\n",
       "      <td>-1.160</td>\n",
       "      <td>10.254</td>\n",
       "      <td>7.111</td>\n",
       "      <td>0.559</td>\n",
       "      <td>-8.708</td>\n",
       "      <td>-12.451</td>\n",
       "      <td>-3.886</td>\n",
       "      <td>-5.290</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-7.924</td>\n",
       "      <td>-2.706</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>co2a0000364</td>\n",
       "      <td>-2.574</td>\n",
       "      <td>5.717</td>\n",
       "      <td>1.149</td>\n",
       "      <td>-2.594</td>\n",
       "      <td>-1.658</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>-1.048</td>\n",
       "      <td>0.478</td>\n",
       "      <td>-1.068</td>\n",
       "      <td>5.809</td>\n",
       "      <td>-5.361</td>\n",
       "      <td>-1.465</td>\n",
       "      <td>0.336</td>\n",
       "      <td>-4.374</td>\n",
       "      <td>-8.270</td>\n",
       "      <td>0.702</td>\n",
       "      <td>1.678</td>\n",
       "      <td>2.492</td>\n",
       "      <td>6.490</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>-1.811</td>\n",
       "      <td>-4.232</td>\n",
       "      <td>-4.293</td>\n",
       "      <td>-4.262</td>\n",
       "      <td>-3.815</td>\n",
       "      <td>-5.778</td>\n",
       "      <td>-7.243</td>\n",
       "      <td>-9.084</td>\n",
       "      <td>-11.383</td>\n",
       "      <td>-12.604</td>\n",
       "      <td>-2.828</td>\n",
       "      <td>7.558</td>\n",
       "      <td>1.211</td>\n",
       "      <td>-12.705</td>\n",
       "      <td>-1.048</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-5.280</td>\n",
       "      <td>2.187</td>\n",
       "      <td>0.966</td>\n",
       "      <td>-1.597</td>\n",
       "      <td>-1.678</td>\n",
       "      <td>-11.027</td>\n",
       "      <td>-1.424</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>-3.571</td>\n",
       "      <td>-8.850</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>1.567</td>\n",
       "      <td>-2.777</td>\n",
       "      <td>-4.435</td>\n",
       "      <td>-5.351</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.305</td>\n",
       "      <td>1.465</td>\n",
       "      <td>-2.655</td>\n",
       "      <td>0.559</td>\n",
       "      <td>-9.196</td>\n",
       "      <td>-11.963</td>\n",
       "      <td>-4.862</td>\n",
       "      <td>-5.290</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-3.042</td>\n",
       "      <td>1.689</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>co2a0000364</td>\n",
       "      <td>5.239</td>\n",
       "      <td>7.670</td>\n",
       "      <td>14.821</td>\n",
       "      <td>-4.547</td>\n",
       "      <td>-0.682</td>\n",
       "      <td>-0.824</td>\n",
       "      <td>-0.559</td>\n",
       "      <td>0.966</td>\n",
       "      <td>-3.510</td>\n",
       "      <td>3.367</td>\n",
       "      <td>-5.361</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.641</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-3.876</td>\n",
       "      <td>6.561</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>2.981</td>\n",
       "      <td>-0.346</td>\n",
       "      <td>2.655</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>-1.322</td>\n",
       "      <td>-3.255</td>\n",
       "      <td>-3.805</td>\n",
       "      <td>-2.797</td>\n",
       "      <td>-1.373</td>\n",
       "      <td>-1.872</td>\n",
       "      <td>-4.801</td>\n",
       "      <td>-7.131</td>\n",
       "      <td>-7.965</td>\n",
       "      <td>-8.698</td>\n",
       "      <td>1.567</td>\n",
       "      <td>19.277</td>\n",
       "      <td>10.488</td>\n",
       "      <td>-7.823</td>\n",
       "      <td>-3.977</td>\n",
       "      <td>12.787</td>\n",
       "      <td>-3.815</td>\n",
       "      <td>4.629</td>\n",
       "      <td>6.826</td>\n",
       "      <td>-3.062</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-6.632</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-1.383</td>\n",
       "      <td>-1.129</td>\n",
       "      <td>-1.526</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>2.055</td>\n",
       "      <td>-2.289</td>\n",
       "      <td>-2.482</td>\n",
       "      <td>-4.374</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.305</td>\n",
       "      <td>5.371</td>\n",
       "      <td>-7.050</td>\n",
       "      <td>0.559</td>\n",
       "      <td>-6.755</td>\n",
       "      <td>-8.545</td>\n",
       "      <td>-3.886</td>\n",
       "      <td>-4.313</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>4.771</td>\n",
       "      <td>5.595</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>co2a0000364</td>\n",
       "      <td>11.587</td>\n",
       "      <td>9.623</td>\n",
       "      <td>20.681</td>\n",
       "      <td>-5.035</td>\n",
       "      <td>2.248</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.905</td>\n",
       "      <td>1.943</td>\n",
       "      <td>-5.463</td>\n",
       "      <td>1.414</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>1.953</td>\n",
       "      <td>-1.129</td>\n",
       "      <td>4.415</td>\n",
       "      <td>1.495</td>\n",
       "      <td>12.421</td>\n",
       "      <td>4.608</td>\n",
       "      <td>5.910</td>\n",
       "      <td>6.002</td>\n",
       "      <td>4.120</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>-0.834</td>\n",
       "      <td>-1.302</td>\n",
       "      <td>-1.851</td>\n",
       "      <td>-0.844</td>\n",
       "      <td>2.045</td>\n",
       "      <td>3.011</td>\n",
       "      <td>-2.360</td>\n",
       "      <td>-3.713</td>\n",
       "      <td>-3.571</td>\n",
       "      <td>-3.326</td>\n",
       "      <td>5.961</td>\n",
       "      <td>23.183</td>\n",
       "      <td>13.906</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>19.135</td>\n",
       "      <td>4.974</td>\n",
       "      <td>8.046</td>\n",
       "      <td>6.337</td>\n",
       "      <td>-3.062</td>\n",
       "      <td>2.716</td>\n",
       "      <td>1.180</td>\n",
       "      <td>2.482</td>\n",
       "      <td>-1.383</td>\n",
       "      <td>2.777</td>\n",
       "      <td>4.822</td>\n",
       "      <td>2.431</td>\n",
       "      <td>1.567</td>\n",
       "      <td>-0.824</td>\n",
       "      <td>0.448</td>\n",
       "      <td>-1.933</td>\n",
       "      <td>2.777</td>\n",
       "      <td>2.258</td>\n",
       "      <td>-12.207</td>\n",
       "      <td>-4.120</td>\n",
       "      <td>1.048</td>\n",
       "      <td>-3.337</td>\n",
       "      <td>-3.662</td>\n",
       "      <td>-1.933</td>\n",
       "      <td>-2.360</td>\n",
       "      <td>-0.478</td>\n",
       "      <td>11.607</td>\n",
       "      <td>9.013</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trial      subject     FP1    FP2  ...    CPZ      nd      Y  alcoholic\n",
       "0      0  co2a0000364  -8.921  0.834  ... -0.478  -8.901 -5.636          1\n",
       "1      0  co2a0000364  -8.433  3.276  ... -0.966  -7.924 -2.706          1\n",
       "2      0  co2a0000364  -2.574  5.717  ... -0.966  -3.042  1.689          1\n",
       "3      0  co2a0000364   5.239  7.670  ... -0.966   4.771  5.595          1\n",
       "4      0  co2a0000364  11.587  9.623  ... -0.478  11.607  9.013          1\n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify the classification of the subject/ alcoholic vs. control from subject string \n",
    "translate_dict = {'a': 1, 'c' : 0}\n",
    "# append classification to a new column\n",
    "df['alcoholic'] = df['subject'].apply(lambda x: translate_dict[x[3]])\n",
    "subject_list = df['subject'].unique()\n",
    "# drop uneccesary column that sometimes appears\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "muDNKq0dtvIr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "num_trials = len(df.groupby(['subject', 'trial']))\n",
    "len_trial = 256\n",
    "# split dataframe by subject/trial into its own container. This allows the CNN to train on each time series\n",
    "# initialize empty arrays \n",
    "X = np.zeros((num_trials, len_trial, 64))\n",
    "y = []\n",
    "i = 0\n",
    "for subject, subject_df in df.groupby(['subject', 'trial']):\n",
    "  X[i,:,:] = subject_df.iloc[:, 2:].drop('alcoholic', axis=1)\n",
    "  i += 1\n",
    "  y.append(subject_df['alcoholic'].values[0])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "oX0Y8WAIt2gI",
    "outputId": "50db8cd5-d492-4eea-9b3b-28edfa3922b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -4.313,  -4.496,  -6.46 , ...,  -1.79 ,  -7.06 ,  -7.507],\n",
       "       [ -6.266,  -6.449,  -6.948, ...,  -1.302,  -8.036,  -7.996],\n",
       "       [ -8.219,  -8.403,  -6.948, ...,  -1.302,  -9.013,  -8.972],\n",
       "       ...,\n",
       "       [ 12.777,  16.5  , -10.854, ...,  -4.232, -34.892, -34.363],\n",
       "       [ 11.8  ,  15.035,  -9.389, ...,  -4.232, -33.915, -33.386],\n",
       "       [ 12.288,  14.058,  -7.924, ...,  -4.232, -32.939, -32.898]])"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PiODUDQzuI_3"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, normalize\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# reshape to a 2d array to apply normalization\n",
    "nsamples, nx, ny = X.shape\n",
    "d2_train_dataset = X.reshape((nsamples,nx*ny))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    d2_train_dataset, y, random_state=1, stratify=y, shuffle=True)\n",
    "\"\"\"\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\"\"\"\n",
    "X_train_scaled = normalize(X_train, axis=0)\n",
    "X_test_scaled = normalize(X_test, axis=0)\n",
    "\n",
    "\n",
    "# one hot encoding for the alcohilic/control classification\n",
    "# Step 1: Label-encode data set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "encoded_y_train = label_encoder.transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Step 2: Convert encoded labels to one-hot-encoding\n",
    "y_train_categorical = to_categorical(encoded_y_train)\n",
    "y_test_categorical = to_categorical(encoded_y_test)\n",
    "input_shape= d2_train_dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "d7C9sWBcuQoH",
    "outputId": "4a97b27d-5fdf-4184-ecb7-c78d74e19108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_6 (Reshape)          (None, 256, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 247, 1000)         641000    \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 238, 160)          1600160   \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 229, 160)          256160    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 76, 160)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 67, 160)           256160    \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 58, 160)           256160    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_5 ( (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 3,010,602\n",
      "Trainable params: 3,010,282\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dropout, BatchNormalization, Reshape\n",
    "model_m = Sequential()\n",
    "# Create model and add layers\n",
    "\n",
    "# reshape to reconstruct the initial shape of the timeseries/probe array\n",
    "model_m.add(Reshape((len_trial, 64), input_shape= input_shape))\n",
    "# convolutional neural etwork arrays\n",
    "model_m.add(Conv1D(160, 10, activation='relu', input_shape=(len_trial, 64)))\n",
    "model_m.add(Conv1D(160, 10, activation='relu'))\n",
    "model_m.add(MaxPooling1D(3))\n",
    "model_m.add(Conv1D(160, 10, activation='relu'))\n",
    "model_m.add(Conv1D(160, 10, activation='relu'))\n",
    "\n",
    "# Average and Dropout layer to prevent over fitting\n",
    "model_m.add(GlobalAveragePooling1D())\n",
    "model_m.add(BatchNormalization())\n",
    "model_m.add(Dropout(0.5))\n",
    "model_m.add(Dense(2, activation='softmax'))\n",
    "print(model_m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "TScEtKyAuXIE",
    "outputId": "a7e0241b-e2fd-4f22-e56d-c7318d92842e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6633 samples, validate on 1659 samples\n",
      "Epoch 1/100\n",
      "6633/6633 [==============================] - 15s 2ms/step - loss: 0.8854 - acc: 0.6115 - val_loss: 2.1140 - val_acc: 0.3689\n",
      "Epoch 2/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.7055 - acc: 0.6391 - val_loss: 0.6646 - val_acc: 0.6558\n",
      "Epoch 3/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.6956 - acc: 0.6578 - val_loss: 0.6202 - val_acc: 0.6829\n",
      "Epoch 4/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.6739 - acc: 0.6677 - val_loss: 0.6543 - val_acc: 0.6456\n",
      "Epoch 5/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.6476 - acc: 0.6849 - val_loss: 0.6104 - val_acc: 0.6860\n",
      "Epoch 6/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.6137 - acc: 0.7062 - val_loss: 0.6814 - val_acc: 0.6239\n",
      "Epoch 7/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.5889 - acc: 0.7191 - val_loss: 3.8539 - val_acc: 0.6353\n",
      "Epoch 8/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.5869 - acc: 0.7130 - val_loss: 0.7315 - val_acc: 0.6468\n",
      "Epoch 9/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.5703 - acc: 0.7205 - val_loss: 0.5882 - val_acc: 0.7143\n",
      "Epoch 10/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.5506 - acc: 0.7357 - val_loss: 0.5549 - val_acc: 0.7263\n",
      "Epoch 11/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.5293 - acc: 0.7476 - val_loss: 0.5525 - val_acc: 0.7269\n",
      "Epoch 12/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.5286 - acc: 0.7500 - val_loss: 0.5554 - val_acc: 0.7125\n",
      "Epoch 13/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.5083 - acc: 0.7588 - val_loss: 0.5933 - val_acc: 0.6799\n",
      "Epoch 14/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.4926 - acc: 0.7657 - val_loss: 0.5987 - val_acc: 0.6860\n",
      "Epoch 15/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.4662 - acc: 0.7815 - val_loss: 0.7135 - val_acc: 0.6540\n",
      "Epoch 16/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.4449 - acc: 0.7966 - val_loss: 2.1160 - val_acc: 0.4274\n",
      "Epoch 17/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.4338 - acc: 0.8005 - val_loss: 0.4882 - val_acc: 0.7571\n",
      "Epoch 18/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.4032 - acc: 0.8189 - val_loss: 0.6239 - val_acc: 0.6860\n",
      "Epoch 19/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.3972 - acc: 0.8223 - val_loss: 0.7024 - val_acc: 0.6594\n",
      "Epoch 20/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.3701 - acc: 0.8408 - val_loss: 0.7093 - val_acc: 0.6588\n",
      "Epoch 21/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.3664 - acc: 0.8366 - val_loss: 0.8329 - val_acc: 0.6432\n",
      "Epoch 22/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.3497 - acc: 0.8473 - val_loss: 0.8295 - val_acc: 0.6468\n",
      "Epoch 23/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.3635 - acc: 0.8414 - val_loss: 0.6854 - val_acc: 0.6829\n",
      "Epoch 24/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.3242 - acc: 0.8607 - val_loss: 0.4320 - val_acc: 0.7830\n",
      "Epoch 25/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.3043 - acc: 0.8717 - val_loss: 0.4936 - val_acc: 0.7486\n",
      "Epoch 26/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.2888 - acc: 0.8826 - val_loss: 5.7355 - val_acc: 0.6353\n",
      "Epoch 27/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.2769 - acc: 0.8801 - val_loss: 0.4864 - val_acc: 0.7577\n",
      "Epoch 28/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.2494 - acc: 0.8973 - val_loss: 0.7552 - val_acc: 0.6872\n",
      "Epoch 29/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.2510 - acc: 0.8994 - val_loss: 0.6442 - val_acc: 0.7233\n",
      "Epoch 30/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.2656 - acc: 0.8850 - val_loss: 0.5409 - val_acc: 0.7348\n",
      "Epoch 31/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.2322 - acc: 0.9029 - val_loss: 0.5541 - val_acc: 0.7438\n",
      "Epoch 32/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.2265 - acc: 0.9100 - val_loss: 0.5223 - val_acc: 0.7794\n",
      "Epoch 33/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.2110 - acc: 0.9144 - val_loss: 0.5763 - val_acc: 0.7571\n",
      "Epoch 34/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.1877 - acc: 0.9257 - val_loss: 0.6665 - val_acc: 0.7288\n",
      "Epoch 35/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.1694 - acc: 0.9305 - val_loss: 0.3929 - val_acc: 0.8463\n",
      "Epoch 36/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.1601 - acc: 0.9352 - val_loss: 0.4473 - val_acc: 0.8168\n",
      "Epoch 37/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.1463 - acc: 0.9420 - val_loss: 0.4798 - val_acc: 0.8041\n",
      "Epoch 38/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.1466 - acc: 0.9411 - val_loss: 0.5708 - val_acc: 0.7890\n",
      "Epoch 39/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.1645 - acc: 0.9361 - val_loss: 0.4192 - val_acc: 0.8300\n",
      "Epoch 40/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.1376 - acc: 0.9466 - val_loss: 0.4869 - val_acc: 0.7987\n",
      "Epoch 41/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.1123 - acc: 0.9588 - val_loss: 0.6551 - val_acc: 0.7613\n",
      "Epoch 42/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.1175 - acc: 0.9531 - val_loss: 0.5074 - val_acc: 0.8204\n",
      "Epoch 43/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.1414 - acc: 0.9454 - val_loss: 0.4550 - val_acc: 0.8481\n",
      "Epoch 44/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.1448 - acc: 0.9448 - val_loss: 0.6459 - val_acc: 0.7722\n",
      "Epoch 45/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.1298 - acc: 0.9487 - val_loss: 1.0137 - val_acc: 0.7697\n",
      "Epoch 46/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.1362 - acc: 0.9459 - val_loss: 0.8136 - val_acc: 0.7999\n",
      "Epoch 47/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.1228 - acc: 0.9534 - val_loss: 0.5340 - val_acc: 0.8143\n",
      "Epoch 48/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.1025 - acc: 0.9613 - val_loss: 0.4183 - val_acc: 0.8571\n",
      "Epoch 49/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.0647 - acc: 0.9796 - val_loss: 0.4243 - val_acc: 0.8535\n",
      "Epoch 50/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.0618 - acc: 0.9816 - val_loss: 0.5659 - val_acc: 0.8198\n",
      "Epoch 51/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.0739 - acc: 0.9750 - val_loss: 0.4735 - val_acc: 0.8590\n",
      "Epoch 52/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.0705 - acc: 0.9754 - val_loss: 0.5223 - val_acc: 0.8366\n",
      "Epoch 53/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.0549 - acc: 0.9815 - val_loss: 0.5459 - val_acc: 0.8499\n",
      "Epoch 54/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0779 - acc: 0.9709 - val_loss: 0.4967 - val_acc: 0.8638\n",
      "Epoch 55/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.0672 - acc: 0.9768 - val_loss: 0.9577 - val_acc: 0.8240\n",
      "Epoch 56/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.0801 - acc: 0.9668 - val_loss: 0.5398 - val_acc: 0.8475\n",
      "Epoch 57/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.0695 - acc: 0.9756 - val_loss: 0.6436 - val_acc: 0.8571\n",
      "Epoch 58/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.0498 - acc: 0.9828 - val_loss: 0.5315 - val_acc: 0.8632\n",
      "Epoch 59/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0407 - acc: 0.9863 - val_loss: 0.5088 - val_acc: 0.8656\n",
      "Epoch 60/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0511 - acc: 0.9830 - val_loss: 0.7945 - val_acc: 0.8246\n",
      "Epoch 61/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0610 - acc: 0.9766 - val_loss: 0.5917 - val_acc: 0.8710\n",
      "Epoch 62/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0526 - acc: 0.9804 - val_loss: 0.6798 - val_acc: 0.8529\n",
      "Epoch 63/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.0350 - acc: 0.9881 - val_loss: 0.6162 - val_acc: 0.8602\n",
      "Epoch 64/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.0403 - acc: 0.9866 - val_loss: 0.6576 - val_acc: 0.8553\n",
      "Epoch 65/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0300 - acc: 0.9907 - val_loss: 0.5998 - val_acc: 0.8571\n",
      "Epoch 66/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.0229 - acc: 0.9938 - val_loss: 0.6231 - val_acc: 0.8565\n",
      "Epoch 67/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.0217 - acc: 0.9940 - val_loss: 0.6598 - val_acc: 0.8620\n",
      "Epoch 68/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.0564 - acc: 0.9799 - val_loss: 0.6190 - val_acc: 0.8716\n",
      "Epoch 69/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0602 - acc: 0.9789 - val_loss: 1.5176 - val_acc: 0.7486\n",
      "Epoch 70/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0496 - acc: 0.9842 - val_loss: 0.7163 - val_acc: 0.8626\n",
      "Epoch 71/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0400 - acc: 0.9852 - val_loss: 0.6715 - val_acc: 0.8523\n",
      "Epoch 72/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0336 - acc: 0.9893 - val_loss: 0.7057 - val_acc: 0.8577\n",
      "Epoch 73/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0208 - acc: 0.9935 - val_loss: 0.6529 - val_acc: 0.8692\n",
      "Epoch 74/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0152 - acc: 0.9964 - val_loss: 0.6235 - val_acc: 0.8740\n",
      "Epoch 75/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0166 - acc: 0.9949 - val_loss: 0.6509 - val_acc: 0.8698\n",
      "Epoch 76/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0113 - acc: 0.9970 - val_loss: 0.5971 - val_acc: 0.8656\n",
      "Epoch 77/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0069 - acc: 0.9982 - val_loss: 0.6169 - val_acc: 0.8752\n",
      "Epoch 78/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0111 - acc: 0.9968 - val_loss: 0.7676 - val_acc: 0.8433\n",
      "Epoch 79/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0110 - acc: 0.9965 - val_loss: 0.8361 - val_acc: 0.8577\n",
      "Epoch 80/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0117 - acc: 0.9967 - val_loss: 0.7447 - val_acc: 0.8656\n",
      "Epoch 81/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0068 - acc: 0.9989 - val_loss: 0.7677 - val_acc: 0.8650\n",
      "Epoch 82/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0045 - acc: 0.9995 - val_loss: 0.6978 - val_acc: 0.8644\n",
      "Epoch 83/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0031 - acc: 0.9997 - val_loss: 0.7166 - val_acc: 0.8638\n",
      "Epoch 84/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0044 - acc: 0.9992 - val_loss: 0.6876 - val_acc: 0.8776\n",
      "Epoch 85/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.0079 - acc: 0.9982 - val_loss: 0.7006 - val_acc: 0.8668\n",
      "Epoch 86/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0064 - acc: 0.9988 - val_loss: 0.7036 - val_acc: 0.8722\n",
      "Epoch 87/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.0045 - acc: 0.9992 - val_loss: 0.6643 - val_acc: 0.8716\n",
      "Epoch 88/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0041 - acc: 0.9994 - val_loss: 0.6930 - val_acc: 0.8776\n",
      "Epoch 89/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.0030 - acc: 0.9992 - val_loss: 0.6899 - val_acc: 0.8674\n",
      "Epoch 90/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0085 - acc: 0.9977 - val_loss: 0.7782 - val_acc: 0.8590\n",
      "Epoch 91/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0124 - acc: 0.9959 - val_loss: 0.7482 - val_acc: 0.8722\n",
      "Epoch 92/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0242 - acc: 0.9914 - val_loss: 0.8208 - val_acc: 0.8626\n",
      "Epoch 93/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0637 - acc: 0.9781 - val_loss: 1.7354 - val_acc: 0.7655\n",
      "Epoch 94/100\n",
      "6633/6633 [==============================] - 14s 2ms/step - loss: 0.0645 - acc: 0.9757 - val_loss: 0.8019 - val_acc: 0.8571\n",
      "Epoch 95/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0569 - acc: 0.9803 - val_loss: 1.0283 - val_acc: 0.8288\n",
      "Epoch 96/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0456 - acc: 0.9845 - val_loss: 0.7185 - val_acc: 0.8571\n",
      "Epoch 97/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0350 - acc: 0.9882 - val_loss: 0.8597 - val_acc: 0.8445\n",
      "Epoch 98/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0215 - acc: 0.9928 - val_loss: 0.7318 - val_acc: 0.8547\n",
      "Epoch 99/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0161 - acc: 0.9952 - val_loss: 0.7283 - val_acc: 0.8626\n",
      "Epoch 100/100\n",
      "6633/6633 [==============================] - 13s 2ms/step - loss: 0.0093 - acc: 0.9976 - val_loss: 0.7252 - val_acc: 0.8638\n"
     ]
    }
   ],
   "source": [
    "from keras import callbacks\n",
    "callbacks_list = [\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='best_model.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "        monitor='val_loss', save_best_only=True),\n",
    "    \n",
    "]\n",
    "\n",
    "model_m.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model_m.fit(X_train,\n",
    "                      y_train_categorical,\n",
    "                      batch_size=400,\n",
    "                      epochs=100,\n",
    "                      callbacks=callbacks_list,\n",
    "                      validation_split=0.2,\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fir-xVwFWWoh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "conv.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
